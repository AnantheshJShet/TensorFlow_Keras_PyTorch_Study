{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST Image classification using CNN in TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import TensorFlow\n",
    "import tensorflow as tf\n",
    "# Import MNIST dataset\n",
    "from tensorflow.examples.tutorials import mnist\n",
    "# Import numpy\n",
    "import numpy as np\n",
    "# Import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Read in MNIST data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data\n",
    "data = mnist.input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shapes of training set\n",
    "print(\"Training set (images) shape: {shape}\".format(shape=data.train.images.shape))\n",
    "print(\"Training set (labels) shape: {shape}\".format(shape=data.train.labels.shape))\n",
    "\n",
    "# Shapes of test set\n",
    "print(\"Test set (images) shape: {shape}\".format(shape=data.test.images.shape))\n",
    "print(\"Test set (labels) shape: {shape}\".format(shape=data.test.labels.shape))\n",
    "\n",
    "image_hgt = 28\n",
    "image_wid = 28\n",
    "n_classes = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictionary of target classes\n",
    "label_dict = {\n",
    "    0: 'Zero',\n",
    "    1: 'One',\n",
    "    2: 'Two',\n",
    "    3: 'Three',\n",
    "    4: 'Four',\n",
    "    5: 'Five',\n",
    "    6: 'Six',\n",
    "    7: 'Seven',\n",
    "    8: 'Eight',\n",
    "    9: 'Nine'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[5,5])\n",
    "\n",
    "# Display first image in training data\n",
    "plt.subplot(121)\n",
    "img = np.reshape(data.train.images[0], (image_hgt,image_wid))\n",
    "lbl = np.argmax(data.train.labels[0,:])\n",
    "plt.imshow(img, cmap='gray')\n",
    "plt.title(\"(Label: \" + str(label_dict[lbl]) + \")\")\n",
    "\n",
    "# Display second image in testing data\n",
    "plt.subplot(122)\n",
    "img = np.reshape(data.test.images[1], (image_hgt,image_wid))\n",
    "lbl = np.argmax(data.test.labels[1,:])\n",
    "plt.imshow(img, cmap='gray')\n",
    "plt.title(\"(Label: \" + str(label_dict[lbl]) + \")\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape training and testing images\n",
    "train_X = data.train.images.reshape(-1, image_hgt, image_wid, 1) # number of samples, length, width, number of channels\n",
    "test_X = data.test.images.reshape(-1, image_hgt, image_wid, 1) # number of samples, length, width, number of channels\n",
    "print(\"Shape of training data: \", train_X.shape)\n",
    "print(\"Shape of testing data: \", test_X.shape)\n",
    "print(\"Type of training data element: \", type(train_X[0][0][0][0]))\n",
    "print(\"Type of testing data element: \", type(test_X[0][0][0][0]))\n",
    "\n",
    "train_Y = data.train.labels\n",
    "test_Y = data.test.labels\n",
    "print(\"Shape of training label: \", train_Y.shape)\n",
    "print(\"Shape of testing lab: \", test_Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "epochs = 50\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1. Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weights dictionary        \n",
    "weight_dict = {\n",
    "    'filterWC1' : tf.get_variable('FWC1', shape=(3,3,1,4), initializer=tf.contrib.layers.xavier_initializer()),\n",
    "    'filterWC2' : tf.get_variable('FWC2', shape=(3,3,4,8), initializer=tf.contrib.layers.xavier_initializer()),\n",
    "    'fullyConnW' : tf.get_variable('FCW', shape=(7*7*8,8), initializer=tf.contrib.layers.xavier_initializer()),\n",
    "    'OutputW' : tf.get_variable('OW', shape=(8,n_classes), initializer=tf.contrib.layers.xavier_initializer())\n",
    "}\n",
    "\n",
    "bias_dict = {\n",
    "    'filterBC1' : tf.get_variable('FBC1', shape=(4), initializer=tf.contrib.layers.xavier_initializer()),\n",
    "    'filterBC2' : tf.get_variable('FBC2', shape=(8), initializer=tf.contrib.layers.xavier_initializer()),\n",
    "    'fullyConnB' : tf.get_variable('FCB', shape=(8), initializer=tf.contrib.layers.xavier_initializer()),\n",
    "    'OutputB' : tf.get_variable('OB', shape=(n_classes), initializer=tf.contrib.layers.xavier_initializer())\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2. Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_net(inp, weight_dict, bias_dict):\n",
    "    # Convolution Layer 1\n",
    "    conv1 = tf.nn.conv2d(inp, weight_dict['filterWC1'], strides = [1, 1, 1, 1], padding = 'SAME')\n",
    "    conv1 = tf.nn.bias_add(conv1, bias_dict['filterBC1'])\n",
    "    conv1 = tf.nn.leaky_relu(conv1, 0.1)\n",
    "    \n",
    "    conv1 = tf.nn.max_pool(conv1, ksize = [1, 2, 2, 1], strides = [1, 2, 2, 1], padding = 'SAME')\n",
    "    \n",
    "    # Convolution Layer 2\n",
    "    conv2 = tf.nn.conv2d(conv1, weight_dict['filterWC2'], strides = [1, 1, 1, 1], padding = 'SAME')\n",
    "    conv2 = tf.nn.bias_add(conv1, bias_dict['filterBC2'])\n",
    "    conv2 = tf.nn.leaky_relu(conv1, 0.1)\n",
    "    \n",
    "    conv2 = tf.nn.max_pool(conv2, ksize = [1, 2, 2, 1], strides = [1, 2, 2, 1], padding = 'SAME')\n",
    "    \n",
    "    # Flatten the convolution layer output\n",
    "    flatten = tf.reshape(conv2, [-1, weight_dict['fullyConnW'].get_shape().as_list()[0]])\n",
    "    \n",
    "    # Fully Connected Layer\n",
    "    fc = tf.matmul(flatten, weight_dict['fullyConnW'])\n",
    "    fc = tf.add(fc, bias_dict['fullyConnB'])\n",
    "    fc = tf.nn.leaky_relu(fc, 0.1)\n",
    "    \n",
    "    # Output, class prediction\n",
    "    out = tf.matmul(fc, weight_dict['OutputW'])\n",
    "    out = tf.add(out, bias_dict['OutputB'])\n",
    "    out = tf.nn.softmax(out);\n",
    "    \n",
    "    return out\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3. Loss and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define placeholders\n",
    "x = tf.placeholder(tf.float32, [None, image_hgt, image_wid, 1])\n",
    "y = tf.placeholder(tf.float32, [None, n_classes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction\n",
    "pred = conv_net(x, weight_dict, bias_dict)\n",
    "\n",
    "# Define the Cost function\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4. Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here you check whether the index of the maximum value of the predicted image \n",
    "# is equal to the actual labelled image and both will be a column vector\n",
    "correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "\n",
    "# Calculate accuracy across all the given images and average them out. \n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. TensorFlow session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # Variable must be initialized before a graph is used for the first time.\n",
    "    sess.run(init)\n",
    "    train_loss_list = []\n",
    "    test_loss_list = []\n",
    "    train_accuracy_list = []\n",
    "    test_accuracy_list = []\n",
    "    summary_writer = tf.summary.FileWriter('./Output', sess.graph)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        for batch in range(len(train_X)//batch_size):\n",
    "            batch_x = train_X[batch*batch_size:min(((batch+1)*batch_size), len(train_X))]\n",
    "            batch_y = train_Y[batch*batch_size:min(((batch+1)*batch_size), len(train_Y))]\n",
    "            #print(batch_x.shape, batch_y.shape)\n",
    "            opt = sess.run(optimizer, feed_dict = {x:batch_x, y:batch_y})\n",
    "            train_loss, train_acc = sess.run([cost, accuracy], feed_dict = {x:batch_x, y:batch_y})\n",
    "            #print(\"Epoch \" + str(epoch) + \", Batch \" + str(batch) + \", Loss = {:.6f}\".format(train_loss) + \", Training Accuracy = {:.5f}\".format(train_acc))\n",
    "            \n",
    "        print(\"Epoch \" + str(epoch) + \", Loss = {:.6f}\".format(train_loss) + \", Training Accuracy = {:.5f}\".format(train_acc))\n",
    "        \n",
    "        \n",
    "        # Calclate accuracy for all test images\n",
    "        test_loss, test_acc = sess.run([cost, accuracy], feed_dict = {x:test_X, y:test_Y})\n",
    "        print(\"Testing Accuracy: {:.5f}\".format(test_acc))\n",
    "        \n",
    "        train_loss_list.append(train_loss)\n",
    "        test_loss_list.append(test_loss)\n",
    "        train_accuracy_list.append(train_acc)\n",
    "        test_accuracy_list.append(test_acc)\n",
    "        \n",
    "    summary_writer.close()        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualize Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(epochs), train_loss_list, 'b', label='Training Loss')\n",
    "plt.plot(range(epochs), test_loss_list, 'r', label='Test Loss')\n",
    "plt.title('Training and Test Loss')\n",
    "plt.xlabel('Epochs ',fontsize=16)\n",
    "plt.ylabel('Loss',fontsize=16)\n",
    "plt.legend()\n",
    "plt.figure()\n",
    "plt.show()\n",
    "\n",
    "plt.plot(range(epochs), train_accuracy_list, 'b', label='Training Accuracy')\n",
    "plt.plot(range(epochs), test_accuracy_list, 'r', label='Test Accuracy')\n",
    "plt.title('Training and Test Accuracy')\n",
    "plt.xlabel('Epochs ',fontsize=16)\n",
    "plt.ylabel('Accuracy',fontsize=16)\n",
    "plt.legend()\n",
    "plt.figure()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
